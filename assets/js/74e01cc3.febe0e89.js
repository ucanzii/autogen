"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1760],{5450:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var o=t(5893),i=t(1151);const s={sidebar_label:"client",title:"oai.client"},r=void 0,a={id:"reference/oai/client",title:"oai.client",description:"OpenAIWrapper Objects",source:"@site/docs/reference/oai/client.md",sourceDirName:"reference/oai",slug:"/reference/oai/client",permalink:"/autogen/docs/reference/oai/client",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/oai/client.md",tags:[],version:"current",frontMatter:{sidebar_label:"client",title:"oai.client"},sidebar:"referenceSideBar",previous:{title:"user_proxy_agent",permalink:"/autogen/docs/reference/agentchat/user_proxy_agent"},next:{title:"completion",permalink:"/autogen/docs/reference/oai/completion"}},l={},c=[{value:"OpenAIWrapper Objects",id:"openaiwrapper-objects",level:2},{value:"__init__",id:"__init__",level:4},{value:"create",id:"create",level:4},{value:"print_usage_summary",id:"print_usage_summary",level:4},{value:"clear_usage_summary",id:"clear_usage_summary",level:4},{value:"cost",id:"cost",level:4},{value:"extract_text_or_completion_object",id:"extract_text_or_completion_object",level:4}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"openaiwrapper-objects",children:"OpenAIWrapper Objects"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class OpenAIWrapper()\n"})}),"\n",(0,o.jsx)(n.p,{children:"A wrapper class for openai client."}),"\n",(0,o.jsx)(n.h4,{id:"__init__",children:"__init__"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def __init__(*,\n             config_list: Optional[List[Dict[str, Any]]] = None,\n             **base_config: Any)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"config_list"})," - a list of config dicts to override the base_config.\nThey can contain additional kwargs as allowed in the ",(0,o.jsx)(n.a,{href:"/docs/reference/oai/client#create",children:"create"})," method. E.g.,"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'config_list=[\n    {\n        "model": "gpt-4",\n        "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),\n        "api_type": "azure",\n        "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),\n        "api_version": "2023-03-15-preview",\n    },\n    {\n        "model": "gpt-3.5-turbo",\n        "api_key": os.environ.get("OPENAI_API_KEY"),\n        "api_type": "open_ai",\n        "base_url": "https://api.openai.com/v1",\n    },\n    {\n        "model": "llama-7B",\n        "base_url": "http://127.0.0.1:8080",\n        "api_type": "open_ai",\n    }\n]\n'})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"base_config"})," - base config. It can contain both keyword arguments for openai client\nand additional kwargs."]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"create",children:"create"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def create(**config: Any) -> ChatCompletion\n"})}),"\n",(0,o.jsx)(n.p,{children:"Make a completion for a given config using openai's clients.\nBesides the kwargs allowed in openai's client, we allow the following additional kwargs.\nThe config in each client will be overridden by the config."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["context (Dict | None): The context to instantiate the prompt or messages. Default to None.\nIt needs to contain keys that are used by the prompt template or the filter function.\nE.g., ",(0,o.jsx)(n.code,{children:'prompt="Complete the following sentence: {prefix}, context={"prefix": "Today I feel"}'}),'.\nThe actual prompt will be:\n"Complete the following sentence: Today I feel".\nMore examples can be found at ',(0,o.jsx)(n.a,{href:"/docs/Use-Cases/enhanced_inference#templating",children:"templating"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"cache_seed"}),' (int | None) for the cache. Default to 41.\nAn integer cache_seed is useful when implementing "controlled randomness" for the completion.\nNone for no caching.']}),"\n",(0,o.jsx)(n.li,{children:"filter_func (Callable | None): A function that takes in the context and the response\nand returns a boolean to indicate whether the response is valid. E.g.,"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def yes_or_no_filter(context, response):\n    return context.get("yes_or_no_choice", False) is False or any(\n        text in ["Yes.", "No."] for text in client.extract_text_or_completion_object(response)\n    )\n'})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"allow_format_str_template (bool | None): Whether to allow format string template in the config. Default to false."}),"\n",(0,o.jsx)(n.li,{children:'api_version (str | None): The api version. Default to None. E.g., "2023-08-01-preview".'}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"print_usage_summary",children:"print_usage_summary"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def print_usage_summary(\n        mode: Union[str, List[str]] = ["actual", "total"]) -> None\n'})}),"\n",(0,o.jsx)(n.p,{children:"Print the usage summary."}),"\n",(0,o.jsx)(n.h4,{id:"clear_usage_summary",children:"clear_usage_summary"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def clear_usage_summary() -> None\n"})}),"\n",(0,o.jsx)(n.p,{children:"Clear the usage summary."}),"\n",(0,o.jsx)(n.h4,{id:"cost",children:"cost"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def cost(response: Union[ChatCompletion, Completion]) -> float\n"})}),"\n",(0,o.jsx)(n.p,{children:"Calculate the cost of the response."}),"\n",(0,o.jsx)(n.h4,{id:"extract_text_or_completion_object",children:"extract_text_or_completion_object"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"@classmethod\ndef extract_text_or_completion_object(\n    cls, response: Union[ChatCompletion, Completion]\n) -> Union[List[str], List[ChatCompletionMessage]]\n"})}),"\n",(0,o.jsx)(n.p,{children:"Extract the text or ChatCompletion objects from a completion or chat response."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"response"})," ",(0,o.jsx)(n.em,{children:"ChatCompletion | Completion"})," - The response from openai."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,o.jsx)(n.p,{children:"A list of text, or a list of ChatCompletion objects if function_call/tool_calls are present."})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>r});var o=t(7294);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);